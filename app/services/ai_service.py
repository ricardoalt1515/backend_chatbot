# app/services/ai_service.py
import logging
import httpx
import json
import os
from typing import List, Dict, Any, Optional

from app.config import settings
from app.models.conversation import Conversation
from app.prompts.main_prompt import get_master_prompt
from app.services.conversation_flow_service import ConversationFlowService
from app.services.proposal_generator import proposal_generator

logger = logging.getLogger("hydrous")


class AIService:
    """Servicio para interactuar con LLMs con manejo mejorado del flujo de conversaci칩n"""

    def __init__(self):
        """Inicializaci칩n del servicio AI"""
        # Cargar datos del cuestionario
        self.questionnaire_data = self._load_questionnaire_data_sync()

        # Cargar el prompt maestro
        self.master_prompt = get_master_prompt(self.questionnaire_data)

        # Inicializar servicio de flujo de conversaci칩n
        self.conversation_flow_service = ConversationFlowService(
            self.questionnaire_data
        )

        # Configuraci칩n de API
        self.api_key = settings.API_KEY
        self.model = settings.MODEL
        self.api_url = settings.API_URL

    def _load_questionnaire_data_sync(self) -> Dict[str, Any]:
        """Carga los datos del cuestionario de forma s칤ncrona"""
        try:
            questionnaire_path = os.path.join(
                os.path.dirname(__file__), "../prompts/questionnaire_complete.json"
            )

            if os.path.exists(questionnaire_path):
                with open(questionnaire_path, "r", encoding="utf-8") as f:
                    return json.load(f)
            else:
                logger.warning(
                    f"No se encontr칩 el archivo de cuestionario en {questionnaire_path}"
                )
                # Devolver una estructura m칤nima para evitar errores
                return {
                    "sectors": ["Industrial", "Comercial"],
                    "subsectors": {
                        "Industrial": ["Textil", "Alimentos y Bebidas"],
                        "Comercial": ["Hotel", "Restaurante"],
                    },
                    "questions": {},
                }
        except Exception as e:
            logger.error(f"Error al cargar datos del cuestionario: {str(e)}")
            return {"sectors": [], "subsectors": {}, "questions": {}}

    async def handle_conversation(
        self, conversation: Conversation, user_message: str = None
    ) -> str:
        """
        Maneja una conversaci칩n y genera una respuesta con mejor control de flujo
        """
        try:
            # Actualizar estado del cuestionario si hay un nuevo mensaje
            if user_message:
                conversation.update_questionnaire_state(
                    user_message, self.questionnaire_data
                )

            # Verificar si el cuestionario est치 completo
            is_complete = self.conversation_flow_service.is_questionnaire_complete(
                conversation
            )

            # Si est치 completo, actualizar metadata
            if is_complete and not conversation.questionnaire_state.is_complete:
                conversation.questionnaire_state.is_complete = True
                conversation.metadata["questionnaire_complete"] = True
                logger.info(
                    f"Cuestionario completo para conversaci칩n {conversation.id}"
                )

            # Obtener pr칩xima pregunta si no est치 completo
            next_question = (
                None
                if is_complete
                else conversation.get_current_question(self.questionnaire_data)
            )

            # Preparar los mensajes para la API con contexto mejorado
            messages = self._prepare_messages_enhanced(
                conversation, user_message, next_question, is_complete
            )

            # Llamar a la API del LLM
            response = await self._call_llm_api(messages)

            # Detectar si el mensaje contiene una propuesta completa
            if "[PROPOSAL_COMPLETE:" in response or self._contains_proposal_markers(
                response
            ):
                conversation.metadata["has_proposal"] = True

                # Usar URL completa en lugar de relativa
                backend_url = (
                    settings.BACKEND_URL or "https://backend-chatbot-owzs.onrender.com"
                )

                # A침adir instrucciones para descargar PDF
                download_instructions = f"""

## 游닌 Descargar Propuesta en PDF

Para descargar esta propuesta en formato PDF, por favor haz clic en el siguiente enlace:

**游녤 [DESCARGAR PROPUESTA EN PDF]({backend_url}/api/chat/{conversation.id}/download-pdf)**
    
Este documento incluye todos los detalles discutidos y puede ser compartido con tu equipo.
"""
                # Solo a침adir las instrucciones si a칰n no est치n presentes
                if "DESCARGAR PROPUESTA EN PDF" not in response:
                    response += download_instructions

            # Si ya hay una propuesta pero no se a침adieron las instrucciones de descarga
            elif (
                conversation.metadata.get("has_proposal", False)
                and "DESCARGAR PROPUESTA" not in response
            ):
                # Verificar si el usuario est치 solicitando la propuesta
                download_keywords = [
                    "pdf",
                    "descargar",
                    "documento",
                    "propuesta",
                    "guardar",
                    "enviar",
                ]
                if user_message and any(
                    keyword in user_message.lower() for keyword in download_keywords
                ):
                    download_instructions = f"""

## 游닌 Descargar Propuesta en PDF

Puedes descargar la propuesta completa en formato PDF haciendo clic en el siguiente enlace:

**游녤 [DESCARGAR PROPUESTA EN PDF]({settings.BACKEND_URL}/api/chat/{conversation.id}/download-pdf)**

Este documento incluye todos los detalles discutidos y puede ser compartido con tu equipo.
"""
                    response += download_instructions

            # Actualizar metadata de la conversaci칩n si necesitamos generar un PDF
            if conversation.metadata.get(
                "has_proposal", False
            ) and not conversation.metadata.get("pdf_requested"):
                conversation.metadata["pdf_requested"] = True

                # Generar PDF en segundo plano si no se ha generado a칰n
                if not conversation.metadata.get("pdf_path"):
                    # Marcar como pendiente para generar PDF
                    conversation.metadata["pdf_pending"] = True

            return response

        except Exception as e:
            logger.error(f"Error en handle_conversation: {str(e)}")
            return "Lo siento, ha ocurrido un error al procesar tu consulta. Por favor, int칠ntalo de nuevo."

    def _prepare_messages_enhanced(
        self,
        conversation: Conversation,
        user_message: str = None,
        next_question: Optional[Dict] = None,
        is_complete: bool = False,
    ) -> List[Dict[str, str]]:
        """Prepara los mensajes para la API con mejor contexto y control de flujo"""
        # Mensaje inicial del sistema con el prompt maestro
        messages = [{"role": "system", "content": self.master_prompt}]

        # A침adir contexto actual como mensaje del sistema
        context_summary = conversation.questionnaire_state.get_context_summary()
        if context_summary:
            context_message = {
                "role": "system",
                "content": f"CONTEXTO ACTUAL:\n{context_summary}\n\nUtiliza esta informaci칩n para personalizar tus respuestas.",
            }
            messages.append(context_message)

        # Obtener hechos relevantes para el sector/industria
        relevant_facts = self.conversation_flow_service.get_relevant_facts(conversation)
        if relevant_facts:
            facts_message = {
                "role": "system",
                "content": f"HECHOS EDUCATIVOS PARA USAR:\n- {relevant_facts[0]}\n- {relevant_facts[1] if len(relevant_facts) > 1 else 'Las soluciones personalizadas pueden reducir significativamente los costos operativos.'}",
            }
            messages.append(facts_message)

        # Informaci칩n espec칤fica de la pr칩xima pregunta
        if next_question:
            question_info = {
                "role": "system",
                "content": f"SIGUIENTE PREGUNTA:\n{next_question['text']}\n\nAseg칰rate de hacer SOLO esta pregunta en tu pr칩xima respuesta.",
            }
            messages.append(question_info)

        # Indicar si es momento de generar la propuesta
        if is_complete:
            # Generar instrucciones espec칤ficas para la propuesta
            proposal_instructions = proposal_generator.generate_proposal_instructions(
                conversation
            )

            completion_message = {
                "role": "system",
                "content": f"El cuestionario est치 COMPLETO. {proposal_instructions}",
            }
            messages.append(completion_message)

        # A침adir mensajes anteriores de la conversaci칩n (limitar para evitar exceder tokens)
        hist_limit = 8  # Ajustar seg칰n necesidad
        for msg in conversation.messages[-hist_limit:]:
            if msg.role != "system":  # No duplicar mensajes del sistema
                messages.append({"role": msg.role, "content": msg.content})

        # Si hay un nuevo mensaje, a침adirlo
        if user_message and (
            not messages
            or messages[-1]["role"] != "user"
            or messages[-1]["content"] != user_message
        ):
            messages.append({"role": "user", "content": user_message})

        return messages

    async def _call_llm_api(self, messages: List[Dict[str, str]]) -> str:
        """Llama a la API del LLM"""
        try:
            # Verificar si hay API key configurada
            if not self.api_key:
                logger.error("API Key no configurada. Revisa tus variables de entorno.")
                return "Error de configuraci칩n: API Key no disponible. Contacta al administrador."

            # Mostrar qu칠 proveedor y modelo estamos usando (para debugging)
            logger.info(
                f"Usando proveedor: {settings.API_PROVIDER} con modelo: {self.model}"
            )

            # Llamar a la API usando httpx
            async with httpx.AsyncClient() as client:
                headers = {
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {self.api_key}",
                }

                payload = {
                    "model": self.model,
                    "messages": messages,
                    "temperature": 0.7,
                    "max_tokens": 2000,  # Aumentando para propuestas completas
                }

                # A침adir timeout m치s largo para evitar problemas con respuestas lentas
                response = await client.post(
                    self.api_url, json=payload, headers=headers, timeout=90.0
                )

                if response.status_code == 200:
                    data = response.json()
                    content = data["choices"][0]["message"]["content"]
                    return content
                elif response.status_code == 401:
                    logger.error(
                        f"Error de autenticaci칩n: API Key inv치lida para {settings.API_PROVIDER}"
                    )
                    return "Lo siento, hay un problema de autenticaci칩n con el servicio. Por favor, contacta al administrador del sistema."
                else:
                    logger.error(
                        f"Error en API LLM: {response.status_code} - {response.text}"
                    )
                    return "Lo siento, ha habido un problema con el servicio. Por favor, int칠ntalo de nuevo m치s tarde."

        except Exception as e:
            logger.error(f"Error en _call_llm_api: {str(e)}")
            return "Lo siento, ha ocurrido un error al comunicarse con el servicio. Por favor, int칠ntalo de nuevo."

    def _contains_proposal_markers(self, text: str) -> bool:
        """Detecta si el texto contiene marcadores de una propuesta completa"""
        # Verificar si contiene las secciones principales de la Propuesta
        key_sections = [
            "Informaci칩n del Cliente",
            "Objetivos del Proyecto",
            "Par치metros de Dise침o",
            "Dise침o del Proceso",
            "Equipamiento Sugerido",
            "CAPEX",
            "OPEX",
            "ROI",
            "An치lisis de Retorno de Inversi칩n",
            "Hydrous Management Group",
        ]

        # Contar cu치ntas secciones est치n presentes
        section_count = sum(1 for section in key_sections if section in text)

        # Si tiene la mayor칤a de las secciones, consideramos que es una propuesta completa
        return section_count >= 5


# Instancia global
ai_service = AIService()
